{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sahithi Paruchuri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring the model\n",
    "batch_size = 32 #The training batch size\n",
    "epochs = 100 #Many epochs better during final training\n",
    "num_samples = 10000 #The sample size we will train on\n",
    "data = \"por.txt\" #The data location in file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go.</th>\n",
       "      <th>Vai.</th>\n",
       "      <th>CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1196331 (alexmarcelo)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>V치.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Oi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corre!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corram!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corre!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corram!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Quem?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Que</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Go.     Vai.  \\\n",
       "0   Go.      V치.   \n",
       "1   Hi.      Oi.   \n",
       "2  Run!   Corre!   \n",
       "3  Run!   Corra!   \n",
       "4  Run!  Corram!   \n",
       "5  Run.   Corre!   \n",
       "6  Run.   Corra!   \n",
       "7  Run.  Corram!   \n",
       "8  Who?    Quem?   \n",
       "9  Who?      Que   \n",
       "\n",
       "  CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1196331 (alexmarcelo)  \n",
       "0  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                                  \n",
       "1  CC-BY 2.0 (France) Attribution: tatoeba.org #5...                                  \n",
       "2  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                                  \n",
       "3  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                                  \n",
       "4  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                                  \n",
       "5  CC-BY 2.0 (France) Attribution: tatoeba.org #4...                                  \n",
       "6  CC-BY 2.0 (France) Attribution: tatoeba.org #4...                                  \n",
       "7  CC-BY 2.0 (France) Attribution: tatoeba.org #4...                                  \n",
       "8  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                                  \n",
       "9  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                                  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_table(\"por.txt\")\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 86\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 45\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning and preparation\n",
    "#Vectorizing the dataset\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data, encoding='utf-8').read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text,_ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "#Creating the dictionary for the characters\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "#Creating the encoder input data\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float64')\n",
    "\n",
    "#Creating the decoder input data\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float64')\n",
    "\n",
    "#Creating the decoder target data\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float64')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 15:37:05.433694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Building the model so that we can test it\n",
    "#Building the encoder\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Building the decoder\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                        initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Defining the model that will turn\n",
    "#`encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 26s 96ms/step - loss: 1.1597 - accuracy: 0.7071 - val_loss: 1.0182 - val_accuracy: 0.7272\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.7631 - accuracy: 0.7826 - val_loss: 0.8183 - val_accuracy: 0.7586\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.6408 - accuracy: 0.8094 - val_loss: 0.7458 - val_accuracy: 0.7766\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.5735 - accuracy: 0.8291 - val_loss: 0.6922 - val_accuracy: 0.7926\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.5250 - accuracy: 0.8432 - val_loss: 0.6485 - val_accuracy: 0.8056\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.4876 - accuracy: 0.8545 - val_loss: 0.6148 - val_accuracy: 0.8188\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.4563 - accuracy: 0.8632 - val_loss: 0.5982 - val_accuracy: 0.8222\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4288 - accuracy: 0.8711 - val_loss: 0.5783 - val_accuracy: 0.8306\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4047 - accuracy: 0.8786 - val_loss: 0.5662 - val_accuracy: 0.8329\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.3822 - accuracy: 0.8849 - val_loss: 0.5537 - val_accuracy: 0.8385\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.3616 - accuracy: 0.8917 - val_loss: 0.5420 - val_accuracy: 0.8425\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.3429 - accuracy: 0.8967 - val_loss: 0.5392 - val_accuracy: 0.8449\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.3258 - accuracy: 0.9015 - val_loss: 0.5345 - val_accuracy: 0.8462\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.3098 - accuracy: 0.9068 - val_loss: 0.5370 - val_accuracy: 0.8467\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.2947 - accuracy: 0.9109 - val_loss: 0.5348 - val_accuracy: 0.8481\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.2812 - accuracy: 0.9150 - val_loss: 0.5325 - val_accuracy: 0.8495\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.2679 - accuracy: 0.9188 - val_loss: 0.5291 - val_accuracy: 0.8498\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.2558 - accuracy: 0.9224 - val_loss: 0.5350 - val_accuracy: 0.8516\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.2441 - accuracy: 0.9259 - val_loss: 0.5453 - val_accuracy: 0.8502\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.2338 - accuracy: 0.9288 - val_loss: 0.5419 - val_accuracy: 0.8518\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.2236 - accuracy: 0.9317 - val_loss: 0.5465 - val_accuracy: 0.8528\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.2140 - accuracy: 0.9345 - val_loss: 0.5519 - val_accuracy: 0.8529\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.2042 - accuracy: 0.9375 - val_loss: 0.5577 - val_accuracy: 0.8520\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.1962 - accuracy: 0.9398 - val_loss: 0.5694 - val_accuracy: 0.8524\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.1879 - accuracy: 0.9423 - val_loss: 0.5735 - val_accuracy: 0.8514\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.1804 - accuracy: 0.9442 - val_loss: 0.5809 - val_accuracy: 0.8510\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1734 - accuracy: 0.9460 - val_loss: 0.5894 - val_accuracy: 0.8523\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.1664 - accuracy: 0.9481 - val_loss: 0.5972 - val_accuracy: 0.8527\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.1600 - accuracy: 0.9501 - val_loss: 0.6043 - val_accuracy: 0.8519\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.1543 - accuracy: 0.9519 - val_loss: 0.6139 - val_accuracy: 0.8505\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.1484 - accuracy: 0.9537 - val_loss: 0.6242 - val_accuracy: 0.8510\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.1425 - accuracy: 0.9554 - val_loss: 0.6234 - val_accuracy: 0.8504\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.1375 - accuracy: 0.9571 - val_loss: 0.6328 - val_accuracy: 0.8501\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.1325 - accuracy: 0.9584 - val_loss: 0.6447 - val_accuracy: 0.8495\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.1278 - accuracy: 0.9598 - val_loss: 0.6533 - val_accuracy: 0.8496\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.1231 - accuracy: 0.9612 - val_loss: 0.6600 - val_accuracy: 0.8504\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.1194 - accuracy: 0.9622 - val_loss: 0.6701 - val_accuracy: 0.8488\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.1145 - accuracy: 0.9637 - val_loss: 0.6750 - val_accuracy: 0.8495\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.1109 - accuracy: 0.9649 - val_loss: 0.6761 - val_accuracy: 0.8502\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.1077 - accuracy: 0.9658 - val_loss: 0.6941 - val_accuracy: 0.8485\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.1034 - accuracy: 0.9674 - val_loss: 0.7010 - val_accuracy: 0.8476\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.1003 - accuracy: 0.9679 - val_loss: 0.7048 - val_accuracy: 0.8485\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0970 - accuracy: 0.9689 - val_loss: 0.7169 - val_accuracy: 0.8476\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0936 - accuracy: 0.9699 - val_loss: 0.7153 - val_accuracy: 0.8480\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0918 - accuracy: 0.9707 - val_loss: 0.7328 - val_accuracy: 0.8457\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0882 - accuracy: 0.9715 - val_loss: 0.7365 - val_accuracy: 0.8463\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0859 - accuracy: 0.9722 - val_loss: 0.7444 - val_accuracy: 0.8472\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0835 - accuracy: 0.9732 - val_loss: 0.7466 - val_accuracy: 0.8475\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0808 - accuracy: 0.9739 - val_loss: 0.7573 - val_accuracy: 0.8466\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0785 - accuracy: 0.9744 - val_loss: 0.7679 - val_accuracy: 0.8466\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0765 - accuracy: 0.9751 - val_loss: 0.7751 - val_accuracy: 0.8463\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0744 - accuracy: 0.9760 - val_loss: 0.7905 - val_accuracy: 0.8463\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.7908 - val_accuracy: 0.8466\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0704 - accuracy: 0.9771 - val_loss: 0.7933 - val_accuracy: 0.8469\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0687 - accuracy: 0.9775 - val_loss: 0.7996 - val_accuracy: 0.8459\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0675 - accuracy: 0.9778 - val_loss: 0.8087 - val_accuracy: 0.8477\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0657 - accuracy: 0.9784 - val_loss: 0.8107 - val_accuracy: 0.8463\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0641 - accuracy: 0.9789 - val_loss: 0.8129 - val_accuracy: 0.8467\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0624 - accuracy: 0.9794 - val_loss: 0.8270 - val_accuracy: 0.8452\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0613 - accuracy: 0.9795 - val_loss: 0.8294 - val_accuracy: 0.8454\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0602 - accuracy: 0.9800 - val_loss: 0.8328 - val_accuracy: 0.8445\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0587 - accuracy: 0.9805 - val_loss: 0.8404 - val_accuracy: 0.8457\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0578 - accuracy: 0.9808 - val_loss: 0.8486 - val_accuracy: 0.8450\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0565 - accuracy: 0.9810 - val_loss: 0.8528 - val_accuracy: 0.8442\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0553 - accuracy: 0.9815 - val_loss: 0.8574 - val_accuracy: 0.8454\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0545 - accuracy: 0.9816 - val_loss: 0.8625 - val_accuracy: 0.8452\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0538 - accuracy: 0.9820 - val_loss: 0.8690 - val_accuracy: 0.8452\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0531 - accuracy: 0.9820 - val_loss: 0.8753 - val_accuracy: 0.8451\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0519 - accuracy: 0.9821 - val_loss: 0.8811 - val_accuracy: 0.8445\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.0510 - accuracy: 0.9826 - val_loss: 0.8802 - val_accuracy: 0.8442\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.0505 - accuracy: 0.9827 - val_loss: 0.8830 - val_accuracy: 0.8445\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0492 - accuracy: 0.9831 - val_loss: 0.8941 - val_accuracy: 0.8435\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0489 - accuracy: 0.9831 - val_loss: 0.8896 - val_accuracy: 0.8444\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0479 - accuracy: 0.9833 - val_loss: 0.8996 - val_accuracy: 0.8429\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0475 - accuracy: 0.9837 - val_loss: 0.8968 - val_accuracy: 0.8457\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0465 - accuracy: 0.9839 - val_loss: 0.9056 - val_accuracy: 0.8444\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0460 - accuracy: 0.9842 - val_loss: 0.9106 - val_accuracy: 0.8433\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0457 - accuracy: 0.9840 - val_loss: 0.9187 - val_accuracy: 0.8443\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 0.9163 - val_accuracy: 0.8431\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.0447 - accuracy: 0.9844 - val_loss: 0.9200 - val_accuracy: 0.8432\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.0441 - accuracy: 0.9845 - val_loss: 0.9290 - val_accuracy: 0.8430\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0439 - accuracy: 0.9844 - val_loss: 0.9313 - val_accuracy: 0.8431\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0436 - accuracy: 0.9847 - val_loss: 0.9275 - val_accuracy: 0.8440\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.9369 - val_accuracy: 0.8420\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0423 - accuracy: 0.9850 - val_loss: 0.9370 - val_accuracy: 0.8444\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0419 - accuracy: 0.9852 - val_loss: 0.9476 - val_accuracy: 0.8414\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0415 - accuracy: 0.9850 - val_loss: 0.9435 - val_accuracy: 0.8433\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0413 - accuracy: 0.9853 - val_loss: 0.9436 - val_accuracy: 0.8436\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0410 - accuracy: 0.9853 - val_loss: 0.9559 - val_accuracy: 0.8425\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0406 - accuracy: 0.9855 - val_loss: 0.9490 - val_accuracy: 0.8448\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0405 - accuracy: 0.9854 - val_loss: 0.9519 - val_accuracy: 0.8446\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0401 - accuracy: 0.9855 - val_loss: 0.9583 - val_accuracy: 0.8442\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0397 - accuracy: 0.9854 - val_loss: 0.9697 - val_accuracy: 0.8429\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0391 - accuracy: 0.9858 - val_loss: 0.9665 - val_accuracy: 0.8433\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.0390 - accuracy: 0.9859 - val_loss: 0.9635 - val_accuracy: 0.8433\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0388 - accuracy: 0.9857 - val_loss: 0.9672 - val_accuracy: 0.8431\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0383 - accuracy: 0.9859 - val_loss: 0.9680 - val_accuracy: 0.8426\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0381 - accuracy: 0.9859 - val_loss: 0.9765 - val_accuracy: 0.8425\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.0383 - accuracy: 0.9861 - val_loss: 0.9723 - val_accuracy: 0.8438\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.0375 - accuracy: 0.9860 - val_loss: 0.9781 - val_accuracy: 0.8446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    }
   ],
   "source": [
    "#Compiling the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Training the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2)\n",
    "\n",
    "#Saving the model\n",
    "model.save('s2s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model('s2s')\n",
    "\n",
    "#Building the encoder model\n",
    "encoder_model = model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_model, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_state_input_h = keras.Input(shape=(256,))\n",
    "decoder_state_input_c = keras.Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "\n",
    "#Reverse-lookup token index to decode sequences back to\n",
    "#something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == \"\\n\" or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: V치.\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: V치.\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Oi.\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Corra!\n",
      "\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Que\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
