{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09310c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the packages that we might need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b7857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data and import\n",
    "\n",
    "#Reading the data\n",
    "lines = pd.read_table('swe.txt', names=['eng', 'swe','other'])[['eng', 'swe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Lowercase all the characters in all the sentences\n",
    "    df.eng = df.eng.apply(lambda x: x.lower())\n",
    "    df.swe = df.swe.apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove all the quote from the sentences\n",
    "    df.eng = df.eng.apply(lambda x: x.replace(\"'\", \"\"))\n",
    "    df.swe = df.swe.apply(lambda x: x.replace(\"'\", \"\"))\n",
    "    df.eng = df.eng.apply(lambda x: x.replace('\"', \"\"))\n",
    "    df.swe = df.swe.apply(lambda x: x.replace('\"', \"\"))\n",
    "\n",
    "    # Remove all the punctuations\n",
    "    exclude = set(string.punctuation)\n",
    "    df.eng = df.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "    df.swe = df.swe.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "    # Remove all the numbers\n",
    "    df.eng = df.eng.apply(lambda x: ''.join(ch for ch in x if not ch.isdigit()))\n",
    "    df.swe = df.swe.apply(lambda x: ''.join(ch for ch in x if not ch.isdigit()))\n",
    "\n",
    "    # Remove all possible extra spaces\n",
    "    df.eng = df.eng.apply(lambda x: x.strip())\n",
    "    df.swe = df.swe.apply(lambda x: x.strip())\n",
    "\n",
    "    # Add the start and end tokens to the Swedish sentences\n",
    "    df.swe = df.swe.apply(lambda x: 'START_ ' + x + ' _END')\n",
    "\n",
    "    return df\n",
    "preprocessed_df = preprocess_data(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d24ba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>swe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10086</th>\n",
       "      <td>i had a good holiday</td>\n",
       "      <td>START_ jag hade en bra ledighet _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16336</th>\n",
       "      <td>why are you angry with him</td>\n",
       "      <td>START_ varför är du arg på honom _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20299</th>\n",
       "      <td>could you drop me off at the library</td>\n",
       "      <td>START_ kan du släppa av mig vid biblioteket _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12639</th>\n",
       "      <td>i like to go to school</td>\n",
       "      <td>START_ jag tycker om att gå i skolan _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>whats your name</td>\n",
       "      <td>START_ vad är ditt namn _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6028</th>\n",
       "      <td>tom cant be dead</td>\n",
       "      <td>START_ tom kan inte vara död _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11358</th>\n",
       "      <td>do you have to go now</td>\n",
       "      <td>START_ måste ni gå nu _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8086</th>\n",
       "      <td>can i get a picture</td>\n",
       "      <td>START_ får jag ta en bild _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21737</th>\n",
       "      <td>whats that how am i supposed to know</td>\n",
       "      <td>START_ ”vad är det där” ”hur ska jag kunna vet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22916</th>\n",
       "      <td>tom didnt know that marys house was so close t...</td>\n",
       "      <td>START_ tom visste inte att marys hus var så nä...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     eng  \\\n",
       "10086                               i had a good holiday   \n",
       "16336                         why are you angry with him   \n",
       "20299               could you drop me off at the library   \n",
       "12639                             i like to go to school   \n",
       "4928                                     whats your name   \n",
       "6028                                    tom cant be dead   \n",
       "11358                              do you have to go now   \n",
       "8086                                 can i get a picture   \n",
       "21737               whats that how am i supposed to know   \n",
       "22916  tom didnt know that marys house was so close t...   \n",
       "\n",
       "                                                     swe  \n",
       "10086               START_ jag hade en bra ledighet _END  \n",
       "16336              START_ varför är du arg på honom _END  \n",
       "20299   START_ kan du släppa av mig vid biblioteket _END  \n",
       "12639          START_ jag tycker om att gå i skolan _END  \n",
       "4928                        START_ vad är ditt namn _END  \n",
       "6028                   START_ tom kan inte vara död _END  \n",
       "11358                         START_ måste ni gå nu _END  \n",
       "8086                      START_ får jag ta en bild _END  \n",
       "21737  START_ ”vad är det där” ”hur ska jag kunna vet...  \n",
       "22916  START_ tom visste inte att marys hus var så nä...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d98637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "all_eng_words = set(word for eng in lines.eng for word in eng.split())\n",
    "\n",
    "# Swedish\n",
    "all_swe_words = set(word for swe in lines.swe for word in swe.split())\n",
    "\n",
    "# Max length of source sequence (English)\n",
    "max_length_src = max(len(sent.split()) for sent in lines.eng)\n",
    "\n",
    "# Max length of target sequence (Swedish)\n",
    "max_length_tar = max(len(sent.split()) for sent in lines.swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66a4a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34737286",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_swe_words))\n",
    "\n",
    "#Number of unique input words\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "\n",
    "#Number of unique output words\n",
    "num_decoder_tokens = len(all_swe_words)\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "\n",
    "#Create a dictionary to convert words to numbers\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "#Create a dictionary to convert numbers to words\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e473ab76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18613,), (4654,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "X, y = lines.eng, lines.swe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04b4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train and test data to pickle for easier reproducibility\n",
    "X_train.to_pickle(\"X_train.pkl\")\n",
    "X_test.to_pickle(\"X_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0179d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X=X_train, y=y_train, batch_size=128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src), dtype=np.float32)\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype=np.float32)\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype=np.float32)\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                # encoder input seq\n",
    "                encoder_input_data[i, :len(input_text.split())] = [input_token_index[word] for word in input_text.split()]\n",
    "                # decoder input seq (without START_)\n",
    "                decoder_input_data[i, :len(target_text.split())-1] = [target_token_index[word] for word in target_text.split()[1:]]\n",
    "                # decoder target seq (without END_)\n",
    "                decoder_target_data[i, :len(target_text.split())-1, :] = tf.keras.utils.to_categorical([target_token_index[word] for word in target_text.split()[:-1]], num_classes=num_decoder_tokens)\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e069481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db776250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 11:24:03.791114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=(None,))\n",
    "enc_emb =  keras.layers.Embedding(num_encoder_tokens+1, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = keras.layers.LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.layers.Input(shape=(None,))\n",
    "dec_emb_layer = keras.layers.Embedding(num_decoder_tokens+1, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0121696",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856da676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/k5dvyn094j7fk5nsdm8z0fvm0000gn/T/ipykernel_16530/4159561959.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 [==============================] - 125s 840ms/step - loss: 0.5605 - acc: 0.1661 - val_loss: 0.4776 - val_acc: 0.1640\n",
      "Epoch 2/50\n",
      "145/145 [==============================] - 133s 921ms/step - loss: 0.4623 - acc: 0.1750 - val_loss: 0.4486 - val_acc: 0.2044\n",
      "Epoch 3/50\n",
      "145/145 [==============================] - 127s 875ms/step - loss: 0.4302 - acc: 0.2318 - val_loss: 0.4156 - val_acc: 0.2563\n",
      "Epoch 4/50\n",
      "145/145 [==============================] - 126s 869ms/step - loss: 0.3989 - acc: 0.2643 - val_loss: 0.3880 - val_acc: 0.2771\n",
      "Epoch 5/50\n",
      "145/145 [==============================] - 116s 802ms/step - loss: 0.3717 - acc: 0.3033 - val_loss: 0.3641 - val_acc: 0.3262\n",
      "Epoch 6/50\n",
      "145/145 [==============================] - 181s 1s/step - loss: 0.3494 - acc: 0.3628 - val_loss: 0.3439 - val_acc: 0.3833\n",
      "Epoch 7/50\n",
      "145/145 [==============================] - 204s 1s/step - loss: 0.3289 - acc: 0.3981 - val_loss: 0.3245 - val_acc: 0.4104\n",
      "Epoch 8/50\n",
      "145/145 [==============================] - 120s 828ms/step - loss: 0.3088 - acc: 0.4244 - val_loss: 0.3052 - val_acc: 0.4387\n",
      "Epoch 9/50\n",
      "145/145 [==============================] - 121s 838ms/step - loss: 0.2891 - acc: 0.4578 - val_loss: 0.2871 - val_acc: 0.4779\n",
      "Epoch 10/50\n",
      "145/145 [==============================] - 261s 2s/step - loss: 0.2711 - acc: 0.4958 - val_loss: 0.2710 - val_acc: 0.5170\n",
      "Epoch 11/50\n",
      "145/145 [==============================] - 447s 3s/step - loss: 0.2550 - acc: 0.5299 - val_loss: 0.2566 - val_acc: 0.5448\n",
      "Epoch 12/50\n",
      "145/145 [==============================] - 130s 897ms/step - loss: 0.2408 - acc: 0.5615 - val_loss: 0.2444 - val_acc: 0.5774\n",
      "Epoch 13/50\n",
      "145/145 [==============================] - 129s 888ms/step - loss: 0.2284 - acc: 0.5913 - val_loss: 0.2331 - val_acc: 0.6040\n",
      "Epoch 14/50\n",
      "145/145 [==============================] - 117s 805ms/step - loss: 0.2168 - acc: 0.6181 - val_loss: 0.2231 - val_acc: 0.6246\n",
      "Epoch 15/50\n",
      "145/145 [==============================] - 117s 806ms/step - loss: 0.2065 - acc: 0.6393 - val_loss: 0.2142 - val_acc: 0.6435\n",
      "Epoch 16/50\n",
      "145/145 [==============================] - 118s 812ms/step - loss: 0.1970 - acc: 0.6589 - val_loss: 0.2056 - val_acc: 0.6634\n",
      "Epoch 17/50\n",
      "145/145 [==============================] - 134s 927ms/step - loss: 0.1880 - acc: 0.6766 - val_loss: 0.1976 - val_acc: 0.6792\n",
      "Epoch 18/50\n",
      "145/145 [==============================] - 130s 895ms/step - loss: 0.1798 - acc: 0.6925 - val_loss: 0.1907 - val_acc: 0.6930\n",
      "Epoch 19/50\n",
      "145/145 [==============================] - 120s 831ms/step - loss: 0.1724 - acc: 0.7077 - val_loss: 0.1841 - val_acc: 0.7057\n",
      "Epoch 20/50\n",
      "145/145 [==============================] - 120s 828ms/step - loss: 0.1656 - acc: 0.7207 - val_loss: 0.1780 - val_acc: 0.7189\n",
      "Epoch 21/50\n",
      "145/145 [==============================] - 115s 797ms/step - loss: 0.1590 - acc: 0.7338 - val_loss: 0.1723 - val_acc: 0.7297\n",
      "Epoch 22/50\n",
      "145/145 [==============================] - 118s 816ms/step - loss: 0.1531 - acc: 0.7463 - val_loss: 0.1672 - val_acc: 0.7419\n",
      "Epoch 23/50\n",
      "145/145 [==============================] - 124s 854ms/step - loss: 0.1472 - acc: 0.7575 - val_loss: 0.1621 - val_acc: 0.7531\n",
      "Epoch 24/50\n",
      "145/145 [==============================] - 117s 809ms/step - loss: 0.1415 - acc: 0.7681 - val_loss: 0.1569 - val_acc: 0.7614\n",
      "Epoch 25/50\n",
      "145/145 [==============================] - 103s 710ms/step - loss: 0.1357 - acc: 0.7777 - val_loss: 0.1520 - val_acc: 0.7707\n",
      "Epoch 26/50\n",
      "145/145 [==============================] - 104s 717ms/step - loss: 0.1304 - acc: 0.7879 - val_loss: 0.1471 - val_acc: 0.7788\n",
      "Epoch 27/50\n",
      "145/145 [==============================] - 105s 727ms/step - loss: 0.1250 - acc: 0.7979 - val_loss: 0.1427 - val_acc: 0.7887\n",
      "Epoch 28/50\n",
      "145/145 [==============================] - 123s 852ms/step - loss: 0.1198 - acc: 0.8072 - val_loss: 0.1383 - val_acc: 0.7954\n",
      "Epoch 29/50\n",
      "145/145 [==============================] - 126s 869ms/step - loss: 0.1152 - acc: 0.8157 - val_loss: 0.1342 - val_acc: 0.8040\n",
      "Epoch 30/50\n",
      "145/145 [==============================] - 133s 920ms/step - loss: 0.1107 - acc: 0.8245 - val_loss: 0.1304 - val_acc: 0.8107\n",
      "Epoch 31/50\n",
      "145/145 [==============================] - 133s 920ms/step - loss: 0.1064 - acc: 0.8324 - val_loss: 0.1272 - val_acc: 0.8191\n",
      "Epoch 32/50\n",
      "145/145 [==============================] - 123s 846ms/step - loss: 0.1025 - acc: 0.8400 - val_loss: 0.1237 - val_acc: 0.8255\n",
      "Epoch 33/50\n",
      "145/145 [==============================] - 120s 831ms/step - loss: 0.0986 - acc: 0.8474 - val_loss: 0.1205 - val_acc: 0.8317\n",
      "Epoch 34/50\n",
      "145/145 [==============================] - 127s 877ms/step - loss: 0.0951 - acc: 0.8537 - val_loss: 0.1176 - val_acc: 0.8375\n",
      "Epoch 35/50\n",
      "145/145 [==============================] - 129s 890ms/step - loss: 0.0918 - acc: 0.8598 - val_loss: 0.1150 - val_acc: 0.8419\n",
      "Epoch 36/50\n",
      "145/145 [==============================] - 395s 3s/step - loss: 0.0887 - acc: 0.8658 - val_loss: 0.1125 - val_acc: 0.8474\n",
      "Epoch 37/50\n",
      "145/145 [==============================] - 106s 730ms/step - loss: 0.0857 - acc: 0.8708 - val_loss: 0.1100 - val_acc: 0.8502\n",
      "Epoch 38/50\n",
      "145/145 [==============================] - 112s 774ms/step - loss: 0.0829 - acc: 0.8759 - val_loss: 0.1079 - val_acc: 0.8552\n",
      "Epoch 39/50\n",
      "145/145 [==============================] - 121s 836ms/step - loss: 0.0802 - acc: 0.8803 - val_loss: 0.1059 - val_acc: 0.8586\n",
      "Epoch 40/50\n",
      "145/145 [==============================] - 120s 828ms/step - loss: 0.0777 - acc: 0.8843 - val_loss: 0.1038 - val_acc: 0.8611\n",
      "Epoch 41/50\n",
      "145/145 [==============================] - 121s 832ms/step - loss: 0.0754 - acc: 0.8879 - val_loss: 0.1018 - val_acc: 0.8636\n",
      "Epoch 42/50\n",
      "145/145 [==============================] - 125s 865ms/step - loss: 0.0730 - acc: 0.8917 - val_loss: 0.0999 - val_acc: 0.8664\n",
      "Epoch 43/50\n",
      "145/145 [==============================] - 124s 855ms/step - loss: 0.0707 - acc: 0.8951 - val_loss: 0.0981 - val_acc: 0.8696\n",
      "Epoch 44/50\n",
      "145/145 [==============================] - 108s 745ms/step - loss: 0.0684 - acc: 0.8982 - val_loss: 0.0964 - val_acc: 0.8731\n",
      "Epoch 45/50\n",
      "145/145 [==============================] - 100s 692ms/step - loss: 0.0663 - acc: 0.9016 - val_loss: 0.0947 - val_acc: 0.8757\n",
      "Epoch 46/50\n",
      "145/145 [==============================] - 94s 646ms/step - loss: 0.0645 - acc: 0.9045 - val_loss: 0.0931 - val_acc: 0.8773\n",
      "Epoch 47/50\n",
      "145/145 [==============================] - 93s 644ms/step - loss: 0.0626 - acc: 0.9074 - val_loss: 0.0918 - val_acc: 0.8797\n",
      "Epoch 48/50\n",
      "145/145 [==============================] - 95s 653ms/step - loss: 0.0611 - acc: 0.9097 - val_loss: 0.0907 - val_acc: 0.8822\n",
      "Epoch 49/50\n",
      "145/145 [==============================] - 95s 657ms/step - loss: 0.0594 - acc: 0.9119 - val_loss: 0.0895 - val_acc: 0.8840\n",
      "Epoch 50/50\n",
      "145/145 [==============================] - 97s 668ms/step - loss: 0.0578 - acc: 0.9146 - val_loss: 0.0882 - val_acc: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9c0035ff0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
    "                    validation_steps=val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff58cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model weights\n",
    "model.save_weights('word_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "446aaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "#Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = keras.models.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bb29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop word.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d61a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generate_batch(X_test, y_test, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33fd143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 692ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Input Question/English sentence: i wonder why tom is naked\n",
      "Actual Answer/Swedish translation:  jag undrar varför tom är naken \n",
      "Predicted Answer/Swedish translation:  START_ per sött innan lat innan lat innan lat i\n",
      "\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Input Question/English sentence: ill wait a week\n",
      "Actual Answer/Swedish translation:  jag ska vänta en vecka \n",
      "Predicted Answer/Swedish translation:  START_ smakar del smakar del varje del varje del v\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Question/English sentence: tom opened his suitcase\n",
      "Actual Answer/Swedish translation:  tom öppnade sin resväska \n",
      "Predicted Answer/Swedish translation:  START_ bröt fattig varje ung varje ung varje ung v\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Input Question/English sentence: dont let him down\n",
      "Actual Answer/Swedish translation:  gör honom inte besviken \n",
      "Predicted Answer/Swedish translation:  START_ mannen skillnad mannen skillnad föll skil\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Input Question/English sentence: you couldve gone\n",
      "Actual Answer/Swedish translation:  ni kunde ha stuckit \n",
      "Predicted Answer/Swedish translation:  START_ nytt fem gånger fem gånger fem gånger fem gå\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Question/English sentence: what time is it now in boston\n",
      "Actual Answer/Swedish translation:  vad är klockan i boston nu \n",
      "Predicted Answer/Swedish translation:  START_ bröt fem åt fem åt fem åt fem åt fem åt\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Input Question/English sentence: can i order now\n",
      "Actual Answer/Swedish translation:  kan jag beställa nu \n",
      "Predicted Answer/Swedish translation:  START_ börjar människa börjar människa varje männ\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Input Question/English sentence: tom drove back to the farm\n",
      "Actual Answer/Swedish translation:  tom körde tillbaka till bondgården \n",
      "Predicted Answer/Swedish translation:  START_ byggnaden människa mannen människa mannen männ\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Input Question/English sentence: tom says he doesnt have any enemies\n",
      "Actual Answer/Swedish translation:  tom säger att han inte har några fiender \n",
      "Predicted Answer/Swedish translation:  START_ några passar några passar några passar n\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Input Question/English sentence: i looked up the word in the dictionary\n",
      "Actual Answer/Swedish translation:  jag slog upp ordet i ordboken \n",
      "Predicted Answer/Swedish translation:  START_ mannen människa mannen människa föll männ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# set the number of random samples to generate\n",
    "num_samples = 10\n",
    "\n",
    "# generate random indices to select random samples from test set\n",
    "random_indices = random.sample(range(len(X_test)), num_samples)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    k = random_indices[i]\n",
    "    (input_seq, actual_output), _ = next(test_gen)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Input Question/English sentence:', X_test[k:k+1].values[0])\n",
    "    print('Actual Answer/Swedish translation:', y_test[k:k+1].values[0][6:-4])\n",
    "    print('Predicted Answer/Swedish translation:', decoded_sentence[:-4])\n",
    "    print('') # add empty line for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9320ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
